<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>guchiBLO - Research</title><link href="https://guchio3.github.io/guchiBLO/" rel="alternate"></link><link href="https://guchio3.github.io/guchiBLO/feeds/research.atom.xml" rel="self"></link><id>https://guchio3.github.io/guchiBLO/</id><updated>2017-09-24T10:00:00+09:00</updated><entry><title>Enriching Word Vectors with Subword Information</title><link href="https://guchio3.github.io/guchiBLO/Enriching_Word_Vectors_with_Subword_Information.html" rel="alternate"></link><published>2017-09-24T10:00:00+09:00</published><updated>2017-09-24T10:00:00+09:00</updated><author><name>guchio3</name></author><id>tag:guchio3.github.io,2017-09-24:/guchiBLO/Enriching_Word_Vectors_with_Subword_Information.html</id><summary type="html">&lt;p&gt;構成文字を考慮した新しい単語の分散表現生成手法&lt;/p&gt;</summary><content type="html">&lt;h1&gt;Outline&lt;/h1&gt;
&lt;p&gt;単語を分散表現する場合、Word2vec などは単語を最小単位として扱った学習によりこれを実現するが、この論文の提案手法では Subword (部分語) に基づいてこれを行う。&lt;/p&gt;
&lt;p&gt;具体的には各単語をその単語中の character n-grams の集合により表現し (一方 Word2vec では word n-grams を使用しており、何を基準とした n-gram かは意識する必要あり)、各 subword に対応するベクトルの和によって単語全体を表すベクトルを表す。 &lt;/p&gt;
&lt;p&gt;これには次のようなメリットがある。 &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Prefix や Suffix といった形態素情報を加味した分散表現が可能&lt;/li&gt;
&lt;li&gt;未知語に対応&lt;/li&gt;
&lt;li&gt;学習が高速... (なぜ高速か、どういう意味で高速かは理解できていない)&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h1&gt;Contents&lt;/h1&gt;
&lt;p&gt;提案手法は非常にシンプル (これもこの手法の良い点の一つ)。以下にその詳細を示す。&lt;/p&gt;
&lt;h3&gt;Skip-gram&lt;/h3&gt;
&lt;p&gt;この論文の提案手法による学習は Skip-gram (&lt;a href="https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf"&gt;Mikilov et al., 2013&lt;/a&gt;) ベースで行われる。&lt;/p&gt;
&lt;p&gt;Skip-gram は、&lt;strong&gt;単語はその文章中での使われ方、つまりその前後にどのような単語が出現するかによりその意味が定義される&lt;/strong&gt;というアイデアに基づいて単語の分散表現を学習する手法である。Skip-gram については&lt;a href="https://google.com"&gt;~さんのブログ&lt;/a&gt;でわかりやすい説明をして下さっている。&lt;/p&gt;
&lt;p&gt;例えば apple, egg そして umbrella という３つの単語について考える。apple と egg は食べ物という意味である程度親しい意味を持っており、これらはどちらも "I ate an * for breakfast." の (* は apple または egg) という使われ方をされうる。一方、"I ate an umbrella for breakfast" という使われ方は普通されない。すなわち例の様に、似ている意味を持つ単語ほど似ている使い方をされやすい、つまり前後に似た単語が現れやすい。(この例はあまり良くなさそう...)&lt;/p&gt;
&lt;p&gt;このアイデアに基づき Skip-gram では文章、つまり単語系列 &lt;span class="math"&gt;\(w_1, w_2, ..., w_n\)&lt;/span&gt; 中のある単語 &lt;span class="math"&gt;\(w_t\)&lt;/span&gt; の分散表現を学習する際、&lt;span class="math"&gt;\(w_t\)&lt;/span&gt; のベクトル表現が以下の式を満たすように学習が行われる。
&lt;/p&gt;
&lt;div class="math"&gt;$$ \sin{a} $$&lt;/div&gt;
&lt;p&gt;
ここで、~。&lt;/p&gt;
&lt;p&gt;しかし Skip-gram は単語を最小単位とした学習を行うため、次の２つの問題を抱える。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Prefix や Suffix といった形態素情報を加味できない&lt;/li&gt;
&lt;li&gt;未知語に非対応&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;この論文で提案する手法では学習時に扱う最小単位を単語から subword へと変更することでこれらの問題に対処できる。&lt;/p&gt;
&lt;h3&gt;Proposal&lt;/h3&gt;
&lt;p&gt;提案手法では各単語をその subword、具体的には character n-grams により表現することを試みる (この論文では 3 ~ 6-gram を使用)。&lt;/p&gt;
&lt;p&gt;例えば、"where" という単語の character 3-grams は 
&lt;/p&gt;
&lt;div class="math"&gt;$$ '&amp;lt;wh', 'whe', 'her', 'ere', 're&amp;gt;' $$&lt;/div&gt;
&lt;p&gt;
の５つである。ここで、&lt;span class="math"&gt;\(&amp;lt;\)&lt;/span&gt; および &lt;span class="math"&gt;\(&amp;gt;\)&lt;/span&gt; は単語の最初および最後を表す文字を示す。&lt;/p&gt;
&lt;p&gt;あとは各 n-gram の分散表現を Skip-gram と同様の学習アルゴリズムにより獲得し、以下のように単語の分散表現を獲得する。
&lt;/p&gt;
&lt;div class="math"&gt;$$ sin{a} $$&lt;/div&gt;
&lt;p&gt;
ここで、~。&lt;/p&gt;
&lt;p&gt;上記の内容から、この提案手法が Prefix や Suffix といった形態素情報を加味した分散表現が可能であり、また各 n-gram の分散表現学習さえ行えてしまえば未知語への対応も可能なことがわかる。&lt;/p&gt;
&lt;hr&gt;
&lt;h1&gt;Discussion&lt;/h1&gt;
&lt;p&gt;初記事です！文章が拙いのは今後改善されていくはずなのでご容赦を...&lt;/p&gt;
&lt;p&gt;Character n-gram ベースで分散表現をするだけという非常にシンプルなアイデアですが、モチベーションはしっかりしていて結果も伴っているのでその点すごいという印象。論文もきれいに書いていて見習いたいと思いました。&lt;br&gt;
ただアイデア自体は何度か見かけたことのあるものなので ACL に通るほどのものかという点には若干疑問も...&lt;/p&gt;
&lt;p&gt;分散表現系の研究は人間の直感を１つの指標として研究が行われる点が私には面白く感じられます。
以前インターンシップで扱ったのですが、当時機械学習を始めたばかりの私にとっては Skip-gram で最終的に利用するのがネットワークの一部のみという点が非常にトリッキーに感じました... (あまり周囲で同様の感想は聞かないですが :P)&lt;/p&gt;
&lt;p&gt;また面白いネタを見つけ次第随時更新していきます！&lt;/p&gt;
&lt;h3&gt;todo&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;tex 対応&lt;/li&gt;
&lt;li&gt;ロゴ対応&lt;/li&gt;
&lt;li&gt;bibtex 対応&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://pdfs.semanticscholar.org/e2db/a792360873aef125572812f3673b1a85d850.pdf"&gt;Original Paper&lt;/a&gt;&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Papers"></category><category term="Machine Learning"></category><category term="Distributed Representation"></category></entry></feed>